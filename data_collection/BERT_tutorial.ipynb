{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\every\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# model.summary()\n",
    "# Now that we have our model, let us create our input sequences from the IMDB reviews dataset.\n",
    "# IMDB reviews dataset is  a large movie review dataset collected and prepared by Andrew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labeledBow.feat', 'neg', 'pos', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
    "                                  origin=URL,\n",
    "                                  untar=True,\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "# Removing the unlabeled reviews\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Create main directory path (\"/aclImdb\")\n",
    "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "# Create sub directory path (\"/aclImdb/train\")\n",
    "train_dir = os.path.join(main_dir, 'train')\n",
    "\n",
    "# Remove unsup folder since this is a supervised learning task\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "\n",
    "# View the final train folder\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\every\\\\OneDrive\\\\바탕 화면\\\\Github\\\\NLP\\\\data_collection\\\\aclImdb\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\every\\OneDrive\\바탕 화면\\Github\\NLP\\data_collection\\BERT_tutorial.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         confidence \u001b[39m=\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m인코딩: \u001b[39m\u001b[39m{\u001b[39;00mencoding\u001b[39m}\u001b[39;00m\u001b[39m, 신뢰도: \u001b[39m\u001b[39m{\u001b[39;00mconfidence\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m detect_encoding(file_path)\n",
      "\u001b[1;32mc:\\Users\\every\\OneDrive\\바탕 화면\\Github\\NLP\\data_collection\\BERT_tutorial.ipynb Cell 3\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_encoding\u001b[39m(file_path):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         raw_data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         result \u001b[39m=\u001b[39m chardet\u001b[39m.\u001b[39mdetect(raw_data)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\every\\\\OneDrive\\\\바탕 화면\\\\Github\\\\NLP\\\\data_collection\\\\aclImdb\\\\train'"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "file_path = r\"C:\\Users\\every\\OneDrive\\바탕 화면\\Github\\NLP\\data_collection\\aclImdb\\train\"\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "        confidence = result['confidence']\n",
    "        print(f\"인코딩: {encoding}, 신뢰도: {confidence}\")\n",
    "\n",
    "detect_encoding(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 29: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\every\\OneDrive\\바탕 화면\\Github\\NLP\\data_collection\\BERT_tutorial.ipynb Cell 3\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We create a training dataset and a validation \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# dataset from our \"aclImdb/train\" directory with a 80/20 split.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39;49mtext_dataset_from_directory(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39maclImdb/train\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m30000\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     subset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m123\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mtext_dataset_from_directory(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39maclImdb/train\u001b[39m\u001b[39m'\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m30000\u001b[39m, validation_split\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m123\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# In this case, batch size is 30,000. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Which means that the machine is going to be trained at once. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/every/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Github/NLP/data_collection/BERT_tutorial.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# This is just an example, not an ideal case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\every\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\text_dataset.py:158\u001b[0m, in \u001b[0;36mtext_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mif\u001b[39;00m seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m1e6\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m file_paths, labels, class_names \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39;49mindex_directory(\n\u001b[0;32m    159\u001b[0m     directory,\n\u001b[0;32m    160\u001b[0m     labels,\n\u001b[0;32m    161\u001b[0m     formats\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m    162\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[0;32m    163\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m    164\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    165\u001b[0m     follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[0;32m    166\u001b[0m )\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m label_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(class_names) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    170\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mWhen passing `label_mode=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`, there must be exactly 2 \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    171\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclass_names. Received: class_names=\u001b[39m\u001b[39m{\u001b[39;00mclass_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\every\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\dataset_utils.py:542\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m     subdirs \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[0;32m    543\u001b[0m         \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    544\u001b[0m             \u001b[39mif\u001b[39;00m subdir\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\every\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:767\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mio.gfile.listdir\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    752\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlist_directory_v2\u001b[39m(path):\n\u001b[0;32m    753\u001b[0m   \u001b[39m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[39m  The list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39m    errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 767\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m    768\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m         node_def\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m         op\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find directory \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m   \u001b[39m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m   \u001b[39m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\every\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:689\u001b[0m, in \u001b[0;36mis_directory\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mgfile.IsDirectory\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_directory\u001b[39m(dirname):\n\u001b[0;32m    681\u001b[0m   \u001b[39m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[0;32m    683\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m    True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m   \u001b[39mreturn\u001b[39;00m is_directory_v2(dirname)\n",
      "File \u001b[1;32mc:\\Users\\every\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:703\u001b[0m, in \u001b[0;36mis_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[39m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \n\u001b[0;32m    696\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[39m  True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 703\u001b[0m   \u001b[39mreturn\u001b[39;00m _pywrap_file_io\u001b[39m.\u001b[39;49mIsDirectory(compat\u001b[39m.\u001b[39;49mpath_to_bytes(path))\n\u001b[0;32m    704\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError:\n\u001b[0;32m    705\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd5 in position 29: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2,\n",
    "    subset='training', seed=123)\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='validation', seed=123)\n",
    "# In this case, batch size is 30,000. \n",
    "# Which means that the machine is going to be trained at once. \n",
    "# This is just an example, not an ideal case.\n",
    "\n",
    "for i in train.take(1):\n",
    "  train_feat = i[0].numpy() # This is the feature for the training dataset.\n",
    "  train_lab = i[1].numpy() # This is the label of the training dataset. \n",
    "  # For supervised learning, the set of features and labels is essential for the training. \n",
    "\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "# T stands for the transpose operator.\n",
    "train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "train['DATA_COLUMN'] = train['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
