{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas 모듈에서는 dataframe을 선언하는 것이 매우 중요하다. 올바른 형태를 가진 dataframe을 가진 상태에서만 그 이후의 행동이 가능해진다. \n",
    "코딩을 하다보면, pandas에서 서로 다른 두 dataframe을 합쳐야 하는 경우를 매우 흔하게 볼 수 있다.\n",
    "이 때 사용할 수 있는 command가 총 세 가지가 있다. \n",
    "1) concatenate - 두 개의 dataframe을 위아래로 이어붙인다고 생각하면 된다. \n",
    "2) join - join 은 두 개의 df를 양 옆으로 이어붙인다. \n",
    "3) merge - merge() 함수는 서로 다른 두 데이터프레임을 각 데이터에 존재하는 고유값을 기준으로 병합할 때 사용한다. \n",
    "pd.merge(df_left,df_right,how='inner', on = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id1': [1, 2, 3, 4, 5], 'name': ['a', 'b', 'c', 'd', 'e'], 'price': [10, 20, 30, 40, 50]}\n",
      "{'id2': [1, 2, 3, 4], 'name': ['a', 'b', 'z', 'z'], 'price': [10, 20, 100, 100]}\n",
      "   id1 namel_  pricel_  id2 namer_  pricer_\n",
      "0    1      a       10  1.0      a     10.0\n",
      "1    2      b       20  2.0      b     20.0\n",
      "2    3      c       30  3.0      z    100.0\n",
      "3    4      d       40  4.0      z    100.0\n",
      "4    5      e       50  NaN    NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "# This is a notebook for the coding implementation.\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "dict_a = {\n",
    "    'id1': [1, 2, 3, 4, 5],\n",
    "    'name': ['a', 'b', 'c', 'd', 'e'],\n",
    "    'price': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "dict_b = {\n",
    "    'id2': [1, 2, 3, 4],\n",
    "    'name': ['a', 'b', 'z', 'z'],\n",
    "    'price': [10, 20, 100, 100]\n",
    "}\n",
    "\n",
    "df_a = pd.DataFrame(dict_a)\n",
    "df_b = pd.DataFrame(dict_b)\n",
    "\n",
    "print(dict_a)\n",
    "print(dict_b)\n",
    "\n",
    "df_test = df_a.join(df_b,lsuffix ='l_',rsuffix ='r_')\n",
    "print(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  4  9\n",
      "1  1  4\n",
      "2  5  6\n",
      "   A  B\n",
      "0  5  9\n",
      "1  2  4\n",
      "2  6  6\n",
      "   A   B\n",
      "0  5  10\n",
      "1  2   5\n",
      "2  6   7\n"
     ]
    }
   ],
   "source": [
    "# This cell is about pandas.apply and lambda. \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([[4,9],[1,4],[5,6]],columns = ['A','B'])\n",
    "print(df)\n",
    "\n",
    "# Let's say we want to apply certain function on the column A. \n",
    "def plus_one(x):\n",
    "    x += 1\n",
    "    return x\n",
    "\n",
    "df['A'] = df['A'].apply(plus_one)\n",
    "print(df)\n",
    "\n",
    "# Motivation - how do we simplify this process?\n",
    "# Initialize the dataframe\n",
    "df = pd.DataFrame([[4,9],[1,4],[5,6]],columns = ['A','B'])\n",
    "df = df.apply(lambda a : a+1 )\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pconda\n"
     ]
    }
   ],
   "source": [
    "# This cell is about string.maketrans()\n",
    "import string\n",
    "\n",
    "obj = 'python'\n",
    "before = 'ython'\n",
    "after = 'conda'\n",
    "sen = obj.maketrans(before,after)\n",
    "print(obj.translate(sen)) # This function is substituting the string with another string.\n",
    "# The prerequisite for this function is the before and after should have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like apple and apple\n"
     ]
    }
   ],
   "source": [
    "# This cell is about re.sub() function. \n",
    "# Example of this function is as follows. \n",
    "import re \n",
    "\n",
    "text = 'I like abple and abple'\n",
    "text_mod = re.sub('abple','apple',text)\n",
    "\n",
    "print(text_mod) # So this function is replacing certain words with another words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish or Periiish\n"
     ]
    }
   ],
   "source": [
    "# This cell is about removing the repeated characters.\n",
    "import re # Regular expression \n",
    "text = 'Publish or Periiish'\n",
    "text_sub = re.sub(r'(.)1+',r'1',text) # +1 means that if the string is repeated more than 1 time, it is removed. \n",
    "#text_sub = re.sub(r'(.)1+',r'1',text)\n",
    "print(text_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cosmosproject.tistory.com/180\n",
    "The site above explains about the syntax of the re.sub() function.\n",
    "https://www.nextree.co.kr/p4327/\n",
    "This site contains nice reference for regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Publish', 'or', 'Perish']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Regular expression is very useful for text processing.\n",
    "# But, it will take some time to be familiar with re.\n",
    "text = 'Publish or Perish!'\n",
    "#tokenizer = RegexpTokenizer(r\"w+\", gaps = True)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# For the second argument, we can specify the criteria for the tokenization.\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\every\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'greatest', 'glori', 'in', 'live', 'lie', 'not', 'in', 'never', 'fall', 'but', 'in', 'rise', 'everi', 'time', 'we', 'fall']\n",
      "['The', 'greatest', 'glory', 'in', 'living', 'lie', 'not', 'in', 'never', 'falling', 'but', 'in', 'rising', 'every', 'time', 'we', 'fall']\n"
     ]
    }
   ],
   "source": [
    "# In this cell, I tried to observe the difference between stemming and lemmatizing.\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "stemmizer = nltk.PorterStemmer()\n",
    "text = 'The greatest glory in living lies not in never falling, but in rising every time we fall.'\n",
    "text = tokenizer.tokenize(text)\n",
    "\n",
    "text_stem = [stemmizer.stem(word) for word in text]\n",
    "# In order to use for loop, we need to put those in the list.\n",
    "print((text_stem))\n",
    "# The result is not 100% accurate.\n",
    "# How about lemmatizer?\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "text_lem = [lemmatizer.lemmatize(word) for word in text]\n",
    "print(text_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'NERS441'), (1, 'NERS444'), (2, 'NERS442'), (3, 'NERS561'), (4, 'NERS544')]\n",
      "course : NERS441, number : 0\n",
      "course : NERS444, number : 1\n",
      "course : NERS442, number : 2\n",
      "course : NERS561, number : 3\n",
      "course : NERS544, number : 4\n"
     ]
    }
   ],
   "source": [
    "# From this cell, I will practice the modules and the functions needed for vectorization. \n",
    "# Enumerate\n",
    "courses = ['NERS441','NERS444','NERS442','NERS561','NERS544']\n",
    "print(list(enumerate(courses)))\n",
    "# The function enumerate will return the index and the element at the same time.\n",
    "# return : [(0, 'NERS441'), (1, 'NERS444'), (2, 'NERS442'), (3, 'NERS561'), (4, 'NERS544')]\n",
    "for n, course in enumerate(courses): # returns index first and then the element.\n",
    "    print(f'course : {course}, number : {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer encoding process is usually done based on the frequency of each word.\n",
    "article = 'Nuclear Bombs save half the lives of Traditional Nuclear Bombs because their radioactive components have half the half life of full half life bombs. This is why science works.'\n",
    "article = tokenizer.tokenize(article)\n",
    "article = [stemmizer.stem(word) for word in article]\n",
    "article = [lemmatizer.lemmatize(word) for word in article]\n",
    "#print(article)\n",
    "\n",
    "vocab = [] # Empty array for integer encoding\n",
    "#vocab = list(enumerate(article))\n",
    "\n",
    "#for n, name in enumerate(article): # n is the index, name is the word.\n",
    "    #print(f'word : {name}, number : {n}')\n",
    "    #frequency = 1\n",
    "    #if name not in vocab: # New words\n",
    "    #    vocab.append(name)\n",
    "    \n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding process \n",
    "# Zero padding\n",
    "# The objective of padding is to make all the arrays to have same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('because', 0),\n",
       " ('bombs', 1),\n",
       " ('but', 2),\n",
       " ('components', 3),\n",
       " ('every', 4),\n",
       " ('fall', 5),\n",
       " ('falling', 6),\n",
       " ('full', 7),\n",
       " ('glory', 8),\n",
       " ('greatest', 9),\n",
       " ('half', 10),\n",
       " ('have', 11),\n",
       " ('in', 12),\n",
       " ('is', 13),\n",
       " ('lies', 14),\n",
       " ('life', 15),\n",
       " ('lives', 16),\n",
       " ('living', 17),\n",
       " ('never', 18),\n",
       " ('not', 19),\n",
       " ('nuclear', 20),\n",
       " ('of', 21),\n",
       " ('radioactive', 22),\n",
       " ('rising', 23),\n",
       " ('save', 24),\n",
       " ('science', 25),\n",
       " ('the', 26),\n",
       " ('their', 27),\n",
       " ('this', 28),\n",
       " ('time', 29),\n",
       " ('traditional', 30),\n",
       " ('we', 31),\n",
       " ('why', 32),\n",
       " ('works', 33)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding \n",
    "# One hot encoding is inefficient in terms of memory. \n",
    "# word2vec encoding - This reflects the similarity in the encoding process.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text = ['The greatest glory in living lies not in never falling, but in rising every time we fall.',\n",
    "        'Nuclear Bombs save half the lives of Traditional Nuclear Bombs because their radioactive components have half the half life of full half life bombs. This is why science works.']\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text) # Input is a collection of documents.\n",
    "print(type(text)) # Input should be list type.\n",
    "# From 'fit' command, the machine is going to study what kind of vocabs are in the data.\n",
    "vectorizer.vocabulary_ # Print the vocab dictionary\n",
    "sorted(vectorizer.vocabulary_.items()) # This will sort the vocab dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'fourth' 'is' 'one' 'or' 'perish' 'publish'\n",
      " 'published' 'second' 'the' 'third' 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is about the understanding of the vectorizer. \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "corpus_train = [\"this is the first document\",\n",
    "\"this document is the second document\",\n",
    "\"And this is the third one.\",\n",
    "\"Is this the fourth published document?\", \"publish or perish\",\"publish or perish\"]\n",
    "# print(corpus_train)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_train) # By this command, it will provide a dictionary of the vocabularies.\n",
    "\n",
    "X = vectorizer.fit_transform(corpus_train) # Vectorizer will make the text into the vector\n",
    "vectorizer.vocabulary_\n",
    "print(vectorizer.get_feature_names_out(corpus_train))\n",
    "X.toarray()\n",
    "# From this array, we notice that the number of nonzero elements are the same as the number of the words in the sentence.\n",
    "# the element 2 means that the term appeared 2 times in a single document.\n",
    "# We noticed that the words are sorted based on the alphabet sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 2 1 2 1 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 3]]\n",
      "   every  hard  hour  how  in  is  success  that  to  waking  work\n",
      "0      1     1     1    1   1   1        2     1   2       1     2\n",
      "1      0     0     0    0   0   0        0     0   0       0     3\n"
     ]
    }
   ],
   "source": [
    "test_data = [\"work hard in every waking hour, that is how to success. work to success!\",\"Work 1,work 2,work 3\"]\n",
    "vectorizer.fit(test_data)\n",
    "result = vectorizer.transform(test_data)\n",
    "print(result.toarray())\n",
    "\n",
    "Doc_Term_Matrix = pd.DataFrame(result.toarray(),columns = vectorizer.get_feature_names_out())\n",
    "print(Doc_Term_Matrix) \n",
    "# Note that in the countvectorizer, it only counts the number of the term in a single sentence.\n",
    "# By this simple experiment, I think the vectorizer is doing pre-processing on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.46979138557992045\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.46979138557992045\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 8)\t0.38408524091481483\n",
      "(4, 9)\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n",
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.91629073, 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
       "       1.91629073, 1.        , 1.91629073, 1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is about Tfidf vectorizer\n",
    "# This will convert text into tfidf feature.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(min_df=0.1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "# Learn the vocaboularies, tf and idf \n",
    "# Returns a document term matrix.\n",
    "vectorizer.get_feature_names_out()\n",
    "Doc_Term_Matrix = pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names_out())\n",
    "print(Doc_Term_Matrix) \n",
    "print(Doc_Term_Matrix.shape)\n",
    "# This will be the tf of each term.\n",
    "# Note that tf is computed as the frequency within a document.\n",
    "vectorizer.idf_ # It will show you the inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-0.5773502691896258\n",
      "  (0, 8)\t-0.5773502691896258\n",
      "  (0, 13)\t0.5773502691896258\n",
      "  (0, 14)\t0.0\n",
      "  (1, 0)\t-0.8164965809277261\n",
      "  (1, 11)\t0.4082482904638631\n",
      "  (1, 13)\t0.4082482904638631\n",
      "  (1, 14)\t0.0\n",
      "  (2, 4)\t-0.7071067811865475\n",
      "  (2, 5)\t0.7071067811865475\n",
      "  (2, 13)\t0.0\n",
      "  (2, 14)\t0.0\n",
      "  (3, 0)\t-0.5773502691896258\n",
      "  (3, 8)\t-0.5773502691896258\n",
      "  (3, 13)\t0.5773502691896258\n",
      "  (3, 14)\t0.0\n",
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "# This cell will explain what hashing vectorizer is. \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "# print(sample_text)\n",
    "vectorizer = HashingVectorizer(n_features= 2**4)\n",
    "Fit_text = vectorizer.fit_transform(corpus)\n",
    "print(Fit_text) \n",
    "print(Fit_text.shape) # (4,16)\n",
    "# (4,16) means that there are 4 documents, and 16 words.\n",
    "# The text is vectorized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35a25c9872cbda683d5908eca59acc1b206505f792393dc9b104ecdd0b60136b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
